Project Overview: 

This project aims to bridge the communication gap between speech and sign language by converting spoken English into sign language gestures and vice versa. It leverages Whisper API from OpenAI for accurate speech-to-text conversion and integrates sign language mappings for accessibility.

Features: 

Speech-to-Text using Whisper API for accurate conversion 

Text-to-Sign Mapping to display corresponding sign language gestures

Bidirectional Conversion: supports both Speech to Sign Language and Sign Language to Speech/Text

Modular design for easily extending vocabulary and gestures

Accessibility focused to support communication for the hearing impaired

Tech Stack: 

Python

Whisper API from OpenAI

OpenCV (for gesture/image processing, if included)

TensorFlow / Keras (for any ML-based gesture recognition)

SpeechRecognition (Python library for audio input handling)
